{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2181c26d-909b-4b58-be17-c9d0aaf110fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025년 2월 2일 아스날과의 경기에서 맨체스터 시티는 1-5로 패했습니다. 이 경기에서의 주요 전술적 요소는 다음과 같습니다.\n",
      "\n",
      "1. **전술적 접근**: 맨체스터 시티는 4-2-3-1 포메이션을 사용했으며, 아스날은 4-3-3 포메이션으로 맞섰습니다. 시티는 중원에서의 점유율을 높이기 위해 두 명의 수비형 미드필더를 배치했지만, 아스날의 압박에 효과적으로 대응하지 못했습니다.\n",
      "\n",
      "2. **공격과 수비의 불균형**: 시티는 14회의 태클을 시도했으나, 성공률은 57.1%로 낮았습니다. 특히, 중원에서의 태클 성공률이 18.2%로 매우 저조했습니다. 이는 아스날의 공격에 대한 저항력이 부족했음을 나타냅니다.\n",
      "\n",
      "3. **xG 분석**: 맨체스터 시티의 기대 골(xG)은 0.8로, 아스날의 1.0에 비해 낮았습니다. 이는 시티가 공격 기회를 창출하는 데 어려움을 겪었음을 보여줍니다. 아스날의 공격에 비해 시티의 공격이 효과적이지 않았고, 이는 경기 결과에 직접적인 영향을 미쳤습니다.\n",
      "\n",
      "4. **점유율**: 시티는 54%의 점유율을 기록했지만, 이는 공격적인 기회를 창출하는 데 충분하지 않았습니다. 점유율이 높았음에도 불구하고, 실질적인 공격 기회로 이어지지 않았습니다.\n",
      "\n",
      "5. **수비적 실수**: 시티는 8회의 블록을 기록했지만, 4회의 실수를 범했습니다. 이는 수비에서의 집중력 부족을 나타내며, 아스날의 공격에 쉽게 뚫렸음을 의미합니다.\n",
      "\n",
      "이러한 요소들을 바탕으로, 다음 경기에서는 중원에서의 압박을 강화하고, 태클 성공률을 높이는 것이 중요합니다. 또한, 공격 기회를 더 효과적으로 창출하기 위한 전술적 조정이 필요할 것입니다."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# 불러올 때는 아래처럼 동일 디렉토리와 임베딩을 지정하면 됨\n",
    "db = Chroma(\n",
    "    persist_directory=\"./chroma_db_9\",\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4) Retriever 설정\n",
    "# ------------------------------------------------\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",           # MMR(Maximal Marginal Relevance)  \n",
    "    search_kwargs={\"k\": 30,      # 유사도가 높은 문서를 30개까지 가져옴\n",
    "                   \"fetch_k\": 30,\n",
    "                   \"lambda_mult\": 0.8}\n",
    ")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 5) 질의 & 문맥 추출\n",
    "# ------------------------------------------------\n",
    "# Pass Types 컬럼이 있다고 가정\n",
    "question = \"Manchester City's  Pass Type information for the match on February 2, 2025.\"\n",
    "\n",
    "# LangChain에서 제공하는 Retrieval 용어와 혼동되지 않도록,\n",
    "# MMRRetriever에서는 `get_relevant_documents` 또는 `get_relevant_text` 같은 메서드가 있으면 사용.\n",
    "# 예시로 `invoke`를 대체한다면:\n",
    "docs = retriever.get_relevant_documents(query=question)\n",
    "\n",
    "# 문맥 통합(단순히 Document의 page_content를 이어붙임)\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 6) LangChain Prompt 구성\n",
    "# ------------------------------------------------\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"당신은 맨체스터 시티의 전술분석관입니다. 2024-2025시즌 맨체스터시티 경기 데이터를 바탕으로 감독님께 도움이 되도록 답변해. 지금 날짜는 2025년 2월 21일이다.\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            {question}\n",
    "            아래의 문맥에 기반하여 답해주세요.\n",
    "            {content}\n",
    "            \"\"\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "message = chat_template.format_messages(\n",
    "    question=question,\n",
    "    content=context\n",
    ")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 7) 모델 호출 & 스트리밍 응답\n",
    "# ------------------------------------------------\n",
    "model = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "for chunk in model.stream(message):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef278a24-25e8-42b7-a5e3-20944edd8234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
