import streamlit as st
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from langchain_core.messages import SystemMessage
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.vectorstores import Chroma

# -------------------------------
# 1) Setup (ì„ë² ë”© ëª¨ë¸ & ChromaDB)
# -------------------------------
embedding_model = OpenAIEmbeddings(model="text-embedding-3-large")
db = Chroma(
    persist_directory="./chroma_db_10",
    embedding_function=embedding_model
)

# MMR Retriever
retriever = db.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 30, "fetch_k": 30, "lambda_mult": 0.8}
)

# -------------------------------
# 2) ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
# -------------------------------
system_template = """\
ë‹¹ì‹ ì€ ë§¨ì²´ìŠ¤í„° ì‹œí‹°ì˜ ì „ìˆ ë¶„ì„ê´€ì…ë‹ˆë‹¤.
2024-2025 ì‹œì¦Œ ë§¨ì²´ìŠ¤í„° ì‹œí‹° ê²½ê¸° ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°ë…ë‹˜ê»˜ ë„ì›€ì´ ë  ë§Œí•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
ì§€ê¸ˆ ë‚ ì§œëŠ” 2025ë…„ 2ì›” 21ì¼ì…ë‹ˆë‹¤.

ë‹µë³€ì€ í•œêµ­ì–´ë¡œ í•˜ì„¸ìš”.

ì•„ë˜ëŠ” ë°ì´í„° í”„ë ˆì„ì˜ ì»¬ëŸ¼(ì¹¼ëŸ¼) ì´ë¦„ê³¼ ê·¸ ì˜ë¯¸ì— ëŒ€í•œ ë²”ë¡€(ì‚¬ì „)ì…ë‹ˆë‹¤.
ì§ˆì˜ì— ë“±ì¥í•˜ëŠ” ì¹¼ëŸ¼ ë˜ëŠ” ì¶•ì•½ì–´ê°€ ìˆë‹¤ë©´ ì´ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ í•´ì„í•˜ì„¸ìš”:

[ë²”ë¡€, ìš©ì–´ ì„¤ëª…]
GF: ë“ì  ìˆ˜, GA: ì‹¤ì  ìˆ˜, xG: ê¸°ëŒ€ ë“ì (Expected Goals), Poss: ì ìœ ìœ¨(%), Ast: ì–´ì‹œìŠ¤íŠ¸ ê°œìˆ˜ ë“±...
"""

# -------------------------------
# 3) Streamlit UI
# -------------------------------
st.title("âš½ï¸ Manchester City Technical AI")
st.write("ğŸ“Š **ë°ì´í„° ê¸°ë°˜ ì¶•êµ¬ ì „ìˆ  ë¶„ì„**")

# Chat UI
if "messages" not in st.session_state:
    st.session_state.messages = []  # ëŒ€í™” ê¸°ë¡ ì €ì¥

# ê¸°ì¡´ ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ì‚¬ìš©ì ì§ˆì˜ ì…ë ¥
user_question = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")

if user_question:
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": user_question})
    with st.chat_message("user"):
        st.markdown(user_question)

    # -------------------------------
    # 4) Retrieverë¡œ ë¬¸ë§¥ ê²€ìƒ‰
    # -------------------------------
    docs = retriever.get_relevant_documents(query=user_question)
    context = "\n\n".join(doc.page_content for doc in docs)

    # -------------------------------
    # 5) í”„ë¡¬í”„íŠ¸ ìƒì„±
    # -------------------------------
    human_template = """\
{question}
ì•„ë˜ì˜ ë¬¸ë§¥ì— ê¸°ë°˜í•˜ì—¬ ë‹µí•´ì£¼ì„¸ìš”.
{context}

ë‹µë³€ ì‹œ, ê°€ëŠ¥í•œ ê²½ìš° í‘œë¡œ ì •ë¦¬í•˜ê±°ë‚˜ ìˆ«ì ì •ë³´ë¥¼ ìš”ì•½í•˜ì—¬ ì œì‹œí•´ì£¼ì„¸ìš”.
"""
    chat_template = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessagePromptTemplate.from_template(human_template)
    ])

    messages = chat_template.format_messages(
        question=user_question,
        context=context
    )

    # -------------------------------
    # 6) LLM í˜¸ì¶œ (ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥)
    # -------------------------------
    model = ChatOpenAI(
        model_name="gpt-4o-mini",
        temperature=0,
        streaming=True
    )

    response_container = st.empty()  # ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ì„ ìœ„í•œ ê³µê°„

    full_answer = ""
    with st.chat_message("assistant"):
        response_placeholder = st.empty()  # AI ì‘ë‹µì„ ë™ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•  ê³µê°„
        for chunk in model.stream(messages):
            full_answer += chunk.content
            response_placeholder.markdown(full_answer)  # ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸

    # AI ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì €ì¥
    st.session_state.messages.append({"role": "assistant", "content": full_answer})
